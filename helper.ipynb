{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88acc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import numpy as np\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c72fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_csv(fn): # get ad's text from csv file, return a dataframe with text, ad_id,ad_url, and report_url\n",
    "    df = pd.read_csv(fn)\n",
    "    df = df[df['ad_type'] == 'Text'].reset_index(drop = True) # only get text ads\n",
    "    df_url = df[['ad_id','ad_url']].reset_index(drop = True) # get the urls\n",
    "    urls = df_url['ad_url'].to_list() # create a list so we could get the report_urls\n",
    "    report_urls = []\n",
    "    for url in urls:\n",
    "        entity_id = url.split('/')[-3]\n",
    "        creative_id = url.split('/')[-1]\n",
    "        report_url = 'https://transparencyreport.google.com/transparencyreport/api/v3/politicalads/creatives/details?entity_id={}&creative_id={}&hl=en'.format(entity_id,creative_id)\n",
    "        report_urls.append(report_url)\n",
    "    ad_text = [] # get texts from reports\n",
    "    for report_url in report_urls:\n",
    "        response = requests.get(report_url)\n",
    "        text = response.text.split('\"]')[0].split('[')[-1]\n",
    "        ad_text.append(text)  \n",
    "    df_text = pd.DataFrame(ad_text).rename(columns = {0:'text'})\n",
    "    df_new = df[df['ad_type']=='Text'][['advertiser_name']].reset_index(drop = True) \n",
    "    df = pd.concat([df_text,df_new],axis=1)\n",
    "    df['platform'] = 'google'\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b928802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset \n",
    "def check_null(df_text):\n",
    "    if df_text['text'].isnull().sum():\n",
    "        print (df_text['text'].isnull().sum())\n",
    "        df_text = df_text[df_text['text'].notnull()]\n",
    "        df_text.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0daaeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that filter the urls and symbols in the text \n",
    "def clean_text(df_text):\n",
    "    def filter_text(x):\n",
    "        url = 'http[s]?://\\S+'\n",
    "        x = re.sub(url,'',x)\n",
    "        x = re.sub(\"[^\\w\\s]\",' ',x) # filter symbols\n",
    "        x = re.sub(\"\\s+\",' ',x)\n",
    "        ls=[w.lower() for w in x.split()] \n",
    "        return ' '.join(ls)\n",
    "    df_text['text'] = df_text['text'].astype(str).apply(lambda x: filter_text(x))\n",
    "    df_text['text'] = df_text['text'].replace('/u0026', ' ')\n",
    "    df_text.drop_duplicates(subset = 'text',keep = 'first', inplace = True)\n",
    "    df_text.reset_index(drop = True,inplace = True)\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a3c8ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function that find the lexicon words in the text\n",
    "def find_words(x,lexicon):\n",
    "    topics= lexicon.keys()  \n",
    "    nlp = spacy.load(\"/usr/local/Cellar/jupyterlab/3.2.9/libexec/lib/python3.9/site-packages/en_core_web_lg/en_core_web_lg-3.2.0/\")\n",
    "    doc = nlp(x) # nlp() is spaCy 2.2 English language model \n",
    "    words= []\n",
    "    for t in topics:\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        terms= lexicon[t]\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        matcher.add(\"TerminologyList\", patterns) # spaCy2.2 phrase matcher\n",
    "        matches = matcher(doc)\n",
    "        for match_id, start,end in matches:\n",
    "            span = doc[start:end]\n",
    "            words.append(span.text)\n",
    "    if words:\n",
    "        words = list(set(words))\n",
    "        return ','.join(words)\n",
    "    else:\n",
    "        return('no words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff10e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the topic in each message\n",
    "def find_topic(x,lexicon):\n",
    "    topics= lexicon.keys()    \n",
    "    if x=='no words':\n",
    "        return ''    \n",
    "    if x != 'no words': \n",
    "        words = x.split(',')\n",
    "        labels = []        \n",
    "        for t in topics:            \n",
    "            terms = lexicon[t]\n",
    "            if set(words)&set(terms):\n",
    "                labels.append(t)                \n",
    "                #l = sorted(labels)        \n",
    "        return  ','.join(sorted(labels))\n",
    "                \n",
    "        #return ','.join(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dc891547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lable(df_text,lexicon):\n",
    "    with open (lexicon) as f:\n",
    "        dic = json.load(f)\n",
    "    df_text['words'] = df_text['text'].astype(str).apply(lambda x: find_words(x,dic))\n",
    "    df_text['m_label'] = df_text['words'].apply(lambda x: find_topic(x,dic))\n",
    "    df_text['m_label'] = df_text['m_label'].apply(lambda x: 'no topic' if x=='' else x)\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f5e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_topic(df_text):\n",
    "    df_tag = df_text['m_label'].value_counts().rename_axis('topics').reset_index(name='counts')\n",
    "    df_tag = df_tag.assign(single_topic=df_tag['topics'].str.split(',')).explode('single_topic').reset_index(drop = True)\n",
    "    df_tag = df_tag.groupby('single_topic').sum().reset_index().sort_values(by = 'counts', ascending = False)\n",
    "    df_tag.reset_index(drop = True, inplace = True)\n",
    "    return df_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9be8a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(df_text):\n",
    "    df_words = df_text['words'].value_counts().rename_axis('words').reset_index(name='counts')\n",
    "    df_words = df_words.assign(single_word=df_words['words'].str.split(',')).explode('single_word').reset_index(drop = True)\n",
    "    df_words = df_words.groupby('single_word').sum().reset_index().sort_values(by = 'counts', ascending = False)\n",
    "    df_words.reset_index(drop = True, inplace = True)\n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be9a9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fb(fn):\n",
    "    facebook_df = pd.read_csv(fn)\n",
    "    facebook_df.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "    facebook_df.rename(columns = {'byline':'advertiser_name'},inplace = True)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    facebook_df['platform'] = 'facebook'\n",
    "    return facebook_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the csv file, get youtube video's ids from all video ads. Add new column 'youtube_id' \n",
    "# before running this function, make sure the csv file includes ad_url\n",
    "def get_youtube_id(fn): \n",
    "    df = pd.read_csv(fn)\n",
    "    df = df[df['ad_type'] == 'Video'].reset_index(drop = True) # only get video ads\n",
    "    df_video = df[['ad_url','advertiser_name','impressions','spend_usd']].reset_index(drop = True) \n",
    "    urls = df_video['ad_url'].to_list() # \n",
    "    youtube_ids = [] # get youtube id\n",
    "    for url in urls:\n",
    "        entity_id = url.split('/')[-3]\n",
    "        creative_id = url.split('/')[-1]\n",
    "        report_url = 'https://transparencyreport.google.com/transparencyreport/api/v3/politicalads/creatives/details?entity_id={}&creative_id={}&hl=en'.format(entity_id,creative_id)\n",
    "        response = requests.get(report_url)\n",
    "        try:\n",
    "            youtube_id = response.text.split('\"')[3]\n",
    "            if len(list(youtube_id)) > 11: # mark ads violating google polices\n",
    "                youtube_id = 'youtube_id not available: this ad violated google ad policies.' \n",
    "            #print(youtube_id + ', ' + creative_id)\n",
    "        except IndexError: # mark ads cannot be loaded\n",
    "            youtube_id = 'youtube_id not available: cannot load the video with this ad_url.'\n",
    "            #print(\"can't load this video, \" + report_url)\n",
    "            pass\n",
    "        youtube_ids.append(youtube_id)\n",
    "    df_video['youtube_id'] = youtube_ids\n",
    "    return df_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all videos, see which are available and drop duplicates. Add a new column video_available\n",
    "def check_video(df_video): \n",
    "  for id in df_video['youtube_id']:\n",
    "    if len(list(id)) == 11:\n",
    "      df_video.drop_duplicates(subset = 'youtube_id',keep = 'first', inplace = True)\n",
    "      df_video.reset_index(drop = True, inplace=True)\n",
    "      yes_video = df_video['youtube_id'].str.len() == 11\n",
    "      df_video['video_available'] = yes_video\n",
    "      return df_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use youtube_id to get captions. Add a new column youtube_captions\n",
    "def get_captions(df_video):\n",
    "  youtube_captions = []\n",
    "  for youtube_id in df_video['youtube_id']:\n",
    "    try:\n",
    "      subs = YouTubeTranscriptApi.get_transcript(youtube_id)\n",
    "      alist = []\n",
    "      for sub in subs:\n",
    "        alist.append(\" \" + sub['text'])\n",
    "      captions = \"\"\n",
    "      for item in alist:\n",
    "        captions += item\n",
    "    except Exception as e:\n",
    "      captions = e   \n",
    "    youtube_captions.append(captions)\n",
    "  #print(len(youtube_captions))\n",
    "  df_video['text'] = youtube_captions\n",
    "  return df_video"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2373ebc037b6b887ce6ae759e146cfea08225ef87b9ff98319d1ded9ec6eadc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('ml-political-ads-gxQjH-2P': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
